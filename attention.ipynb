{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing a very naive attention mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], \n",
    "[0.55, 0.87, 0.66], \n",
    "[0.57, 0.85, 0.64],  \n",
    "[0.22, 0.58, 0.33],  \n",
    "[0.77, 0.25, 0.10],  \n",
    "[0.05, 0.80, 0.55]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n",
    "\n",
    "'''for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i][j] = torch.dot(x_i, x_j)'''\n",
    "\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#context_vec = attn_weights @\n",
    "\n",
    "\n",
    "context_vec = torch.zeros(inputs.shape)\n",
    "\n",
    "\n",
    "'''for i, a_i in enumerate(attn_weights):\n",
    "    for j, x_i in enumerate(inputs):\n",
    "        context_vec[i] += a_i[j] * x_i'''\n",
    "\n",
    "context_vec = attn_weights @ inputs\n",
    "context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coding a better, trainable attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = x_2.shape[0]\n",
    "d_out = x_2.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "w_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # query matrix\n",
    "w_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # key matrix\n",
    "w_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # value matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_q = x_2 @ w_q \n",
    "x_v = x_2 @ w_v\n",
    "x_k = x_2 @ w_k\n",
    "keys = inputs @ w_k # keys for all the input elements\n",
    "values = inputs @ w_v # values for all the input elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4433, 1.1419])\n",
      "tensor([0.4306, 1.4551])\n",
      "tensor([0.3951, 1.0037])\n"
     ]
    }
   ],
   "source": [
    "print(x_k)\n",
    "print(x_q)\n",
    "print(x_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]])\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]])\n"
     ]
    }
   ],
   "source": [
    "print(keys)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores_2 = x_q @ keys.T # dot product of (x^2)_q with (x^i)_k for all ^ as index in the inputs \n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "'''implementing a python class for the above, improved attention'''\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "        queries = x @ self.W_query\n",
    "\n",
    "        attention_weights = torch.softmax((queries @ keys.T) / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "'''re-implementing the class with nn.Linear for better weight initialization '''\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        attention_weights = torch.softmax((queries @ keys.T) / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5337, -0.1051],\n",
      "        [-0.5323, -0.1080],\n",
      "        [-0.5323, -0.1079],\n",
      "        [-0.5297, -0.1076],\n",
      "        [-0.5311, -0.1066],\n",
      "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "wk = sa_v2.W_key.weight.T\n",
    "wq = sa_v2.W_query.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ wk\n",
    "queries = inputs @ wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T\n",
    "type(attn_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4286, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.6286, 0.8000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.4042, 0.5114, 0.5129, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3234, 0.3685, 0.3690, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.2891, 0.2894, 0.0000, 0.2929, 0.0000],\n",
       "        [0.0000, 0.2450, 0.2455, 0.2453, 0.2512, 0.2406]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(attn_scores.shape[0], attn_scores.shape[1]), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "attn_weight = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "dropout = torch.nn.Dropout(0.3)\n",
    "dropout(attn_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_K = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_Q = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_V = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_K(x)\n",
    "        values = self.W_V(x)\n",
    "        queries = self.W_Q(x)\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        attn_scores = attn_scores.masked_fill(self.mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.2636,  0.4184],\n",
      "         [-0.3537,  0.2172],\n",
      "         [-0.3752,  0.1474],\n",
      "         [-0.3611,  0.0971],\n",
      "         [-0.2495,  0.0649],\n",
      "         [-0.3074,  0.0523]],\n",
      "\n",
      "        [[-0.2636,  0.4184],\n",
      "         [-0.3537,  0.2172],\n",
      "         [-0.3752,  0.1474],\n",
      "         [-0.3611,  0.0971],\n",
      "         [-0.2495,  0.0649],\n",
      "         [-0.3074,  0.0523]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs))\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "cv = ca(batch)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "internally, pytorch flattens the tensors (B, T, D) -> (B*T, D), does the operations and then reconstructs the original dims in case of batch operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why one head is not enough\n",
    "\n",
    "- Limited subspace: A single attention head attends in a fixed representation space. It can't simultaneously focus on diverse semantic/structural patterns (like subject-object relationships and polysemy resolution).\n",
    "\n",
    "- Capacity bottleneck: One head has fewer parameters and thus limited capacity to model all the nuances. Multi-head splits the job into specialized roles — one does syntax, another semantics, etc.\n",
    "\n",
    "- Inductive bias: Multi-head attention acts like an ensemble. Each head learns a different perspective. This enforces modularity and improves generalization.\n",
    "\n",
    "- Empirical evidence: Multi-head models like Transformers perform significantly better than single-head counterparts on benchmarks (e.g., BLEU for translation, GLUE for NLP tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)       # splitting the matrix in order to\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)   # generate (B, NUM_HEADS) independent matrices\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2) # of size (NUM_TOKENS, HEAD_DIM) for parallel\n",
    "                                                                                             # computation (hence, multi-head attention)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        attn_scores = attn_scores.masked_fill(self.mask.bool(), -torch.inf)\n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3510, 0.4315],\n",
      "         [0.3571, 0.3429],\n",
      "         [0.3587, 0.3164],\n",
      "         [0.3373, 0.3482],\n",
      "         [0.3329, 0.3713],\n",
      "         [0.3238, 0.3734]],\n",
      "\n",
      "        [[0.3510, 0.4315],\n",
      "         [0.3571, 0.3429],\n",
      "         [0.3587, 0.3164],\n",
      "         [0.3373, 0.3482],\n",
      "         [0.3329, 0.3713],\n",
      "         [0.3238, 0.3734]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0, num_heads=2)\n",
    "cv = mha(batch)\n",
    "print(cv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first n-2 dims define how many independent matrix multiplications happen in parallel.\n",
    "\n",
    "\n",
    "Imagine you have a batch of data or multiple sequences you want to process simultaneously:\n",
    "\n",
    "Those leading dims act as batch dimensions or sequence dims.\n",
    "\n",
    "PyTorch treats each slice along those dims as an independent matrix.\n",
    "\n",
    "It runs the matrix multiplication on all these slices in parallel efficiently on GPU/CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention Example\n",
    "\n",
    "Input Sentence: \"I go to the bank to deposit money\"\n",
    "\n",
    "Each attention head processes the same input, but focuses on different relationships due to different learned projections (W_q, W_k, W_v).\n",
    "\n",
    "##### Head 1: Semantic Disambiguation\n",
    "- Focus: \"bank\" ↔ \"deposit\", \"money\"\n",
    "- Learns that \"bank\" is a financial institution, not a river bank.\n",
    "\n",
    "##### Head 2: Subject Identification\n",
    "- Focus: \"I\" ↔ \"go\", \"to\"\n",
    "- Captures who is acting — the grammatical subject.\n",
    "\n",
    "##### Head 3: Grammatical Structure\n",
    "- Focus: \"to\" ↔ \"go\", \"deposit\"\n",
    "- Models motion or action phrases — verb-preposition-object relationships.\n",
    "\n",
    "\n",
    "Multi-head attention enables different heads to learn specialized roles by attending in different representational subspaces, improving model expressiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
