{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementing a very naive attention mechanism "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89, 0.43, 0.15, 0.89], \n",
    "[0.55, 0.87, 0.66, 0.55, 0.87, 0.66], \n",
    "[0.57, 0.85, 0.64, 0.57, 0.85, 0.64],  \n",
    "[0.22, 0.58, 0.33, 0.57, 0.85, 0.64],  \n",
    "[0.77, 0.25, 0.10, 0.57, 0.85, 0.64],  \n",
    "[0.05, 0.80, 0.55, 0.57, 0.85, 0.64]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.9990, 1.9088, 1.8844, 1.4175, 1.3998, 1.5732],\n",
      "        [1.9088, 2.9900, 2.9508, 2.3188, 2.1824, 2.5619],\n",
      "        [1.8844, 2.9508, 2.9140, 2.2866, 2.1724, 2.5175],\n",
      "        [1.4175, 2.3188, 2.2866, 1.9507, 1.8044, 2.1135],\n",
      "        [1.3998, 2.1824, 2.1724, 1.8044, 2.1224, 1.7505],\n",
      "        [1.5732, 2.5619, 2.5175, 2.1135, 1.7505, 2.4020]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.zeros(inputs.shape[0], inputs.shape[0])\n",
    "\n",
    "'''for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i][j] = torch.dot(x_i, x_j)'''\n",
    "\n",
    "\n",
    "attn_scores = inputs @ inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2190, 0.2001, 0.1953, 0.1224, 0.1203, 0.1430],\n",
       "        [0.0868, 0.2558, 0.2460, 0.1307, 0.1141, 0.1667],\n",
       "        [0.0876, 0.2544, 0.2452, 0.1310, 0.1168, 0.1650],\n",
       "        [0.0907, 0.2233, 0.2162, 0.1545, 0.1335, 0.1818],\n",
       "        [0.0968, 0.2117, 0.2096, 0.1451, 0.1994, 0.1375],\n",
       "        [0.0873, 0.2345, 0.2244, 0.1498, 0.1042, 0.1999]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4422, 0.5884, 0.5830, 0.5353, 0.7007, 0.6987],\n",
       "        [0.4431, 0.6823, 0.5497, 0.5527, 0.7944, 0.6668],\n",
       "        [0.4444, 0.6801, 0.5485, 0.5526, 0.7938, 0.6670],\n",
       "        [0.4309, 0.6601, 0.5308, 0.5528, 0.7910, 0.6671],\n",
       "        [0.4698, 0.6208, 0.5034, 0.5522, 0.7865, 0.6684],\n",
       "        [0.4176, 0.6807, 0.5458, 0.5531, 0.7936, 0.6665]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#context_vec = attn_weights @\n",
    "\n",
    "\n",
    "context_vec = torch.zeros(inputs.shape)\n",
    "\n",
    "\n",
    "'''for i, a_i in enumerate(attn_weights):\n",
    "    for j, x_i in enumerate(inputs):\n",
    "        context_vec[i] += a_i[j] * x_i'''\n",
    "\n",
    "context_vec = attn_weights @ inputs\n",
    "context_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coding a better, trainable attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_2 = inputs[1]\n",
    "d_in = x_2.shape[0]\n",
    "d_out = x_2.shape[0] - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "w_q = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # query matrix\n",
    "w_k = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # key matrix\n",
    "w_v = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False) # value matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_q = x_2 @ w_q \n",
    "x_v = x_2 @ w_v\n",
    "x_k = x_2 @ w_k\n",
    "keys = inputs @ w_k # keys for all the input elements\n",
    "values = inputs @ w_v # values for all the input elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0521, 1.5140, 2.4725, 2.6323, 2.1560])\n",
      "tensor([2.5229, 1.6209, 1.8523, 2.3272, 1.5094])\n",
      "tensor([2.5138, 2.3351, 1.6296, 1.3288, 2.3814])\n"
     ]
    }
   ],
   "source": [
    "print(x_k)\n",
    "print(x_q)\n",
    "print(x_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1804, 0.9781, 1.9330, 2.1744, 1.3799],\n",
      "        [2.0521, 1.5140, 2.4725, 2.6323, 2.1560],\n",
      "        [2.0499, 1.5150, 2.4513, 2.6184, 2.1599],\n",
      "        [1.4823, 1.1638, 1.9629, 2.0296, 1.5527],\n",
      "        [1.7553, 1.4913, 2.1355, 2.1451, 1.7745],\n",
      "        [1.5097, 1.1062, 2.0193, 2.1564, 1.6157]])\n",
      "tensor([[1.7690, 1.7220, 1.1177, 1.1184, 1.8703],\n",
      "        [2.5138, 2.3351, 1.6296, 1.3288, 2.3814],\n",
      "        [2.4855, 2.3294, 1.5882, 1.3147, 2.3576],\n",
      "        [1.8515, 1.6566, 1.4132, 1.2241, 1.8347],\n",
      "        [1.7823, 1.8470, 1.2479, 1.2875, 1.5952],\n",
      "        [2.0740, 1.7629, 1.5386, 1.2204, 2.1008]])\n"
     ]
    }
   ],
   "source": [
    "print(keys)\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_scores_2 = x_q @ keys.T # dot product of (x^2)_q with (x^i)_k for all ^ as index in the inputs \n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k ** 0.5, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0240, 0.4017, 0.3894, 0.0382, 0.0996, 0.0471])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.3661, 2.2168, 1.5507, 1.3050, 2.2475])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "'''implementing a python class for the above, improved attention'''\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        values = x @ self.W_value\n",
    "        queries = x @ self.W_query\n",
    "\n",
    "        attention_weights = torch.softmax((queries @ keys.T) / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2807, 2.1373, 1.5133, 1.2905, 2.1767],\n",
      "        [2.3661, 2.2168, 1.5507, 1.3050, 2.2475],\n",
      "        [2.3639, 2.2148, 1.5497, 1.3047, 2.2457],\n",
      "        [2.3280, 2.1811, 1.5342, 1.2986, 2.2160],\n",
      "        [2.3255, 2.1786, 1.5329, 1.2979, 2.2144],\n",
      "        [2.3397, 2.1922, 1.5394, 1.3008, 2.2253]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttentionV1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "'''re-implementing the class with nn.Linear for better weight initialization '''\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        attention_weights = torch.softmax((queries @ keys.T) / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        context_vec = attention_weights @ values\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2370,  0.2580, -0.3748,  0.3019,  0.1255],\n",
      "        [-0.2384,  0.2577, -0.3741,  0.2964,  0.1261],\n",
      "        [-0.2384,  0.2578, -0.3740,  0.2963,  0.1262],\n",
      "        [-0.2375,  0.2602, -0.3735,  0.2969,  0.1279],\n",
      "        [-0.2381,  0.2593, -0.3730,  0.2963,  0.1277],\n",
      "        [-0.2375,  0.2599, -0.3739,  0.2970,  0.1275]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa_v2 = SelfAttentionV2(d_in, d_out)\n",
    "wk = sa_v2.W_key.weight.T\n",
    "wq = sa_v2.W_query.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = inputs @ wk\n",
    "queries = inputs @ wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores = queries @ keys.T\n",
    "type(attn_scores.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.7123, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.4771, 0.4770, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.3584, 0.0000, 0.3518, 0.0000, 0.0000],\n",
       "        [0.2948, 0.2793, 0.0000, 0.2806, 0.2936, 0.0000],\n",
       "        [0.2378, 0.2435, 0.2432, 0.0000, 0.2312, 0.2385]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(attn_scores.shape[0], attn_scores.shape[1]), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
    "attn_weight = torch.softmax(masked / keys.shape[-1] ** 0.5, dim=1)\n",
    "dropout = torch.nn.Dropout(0.3)\n",
    "dropout(attn_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.W_K = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_Q = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_V = torch.nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_K(x)\n",
    "        values = self.W_V(x)\n",
    "        queries = self.W_Q(x)\n",
    "        attn_scores = queries @ keys.transpose(1,2)\n",
    "        attn_scores = attn_scores.masked_fill(self.mask.bool(), -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim = -1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "\n",
    "        return context_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3642,  0.0227,  0.2398, -0.0169,  0.4638],\n",
      "         [-0.2611,  0.0933,  0.0065, -0.0643,  0.2690],\n",
      "         [-0.2280,  0.1148, -0.0628, -0.0737,  0.2039],\n",
      "         [-0.2038,  0.1369, -0.1097, -0.0577,  0.1818],\n",
      "         [-0.2348,  0.1917, -0.0948, -0.0141,  0.1385],\n",
      "         [-0.2006,  0.1720, -0.1355, -0.0328,  0.1483]],\n",
      "\n",
      "        [[-0.3642,  0.0227,  0.2398, -0.0169,  0.4638],\n",
      "         [-0.2611,  0.0933,  0.0065, -0.0643,  0.2690],\n",
      "         [-0.2280,  0.1148, -0.0628, -0.0737,  0.2039],\n",
      "         [-0.2038,  0.1369, -0.1097, -0.0577,  0.1818],\n",
      "         [-0.2348,  0.1917, -0.0948, -0.0141,  0.1385],\n",
      "         [-0.2006,  0.1720, -0.1355, -0.0328,  0.1483]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch = torch.stack((inputs, inputs))\n",
    "context_length = batch.shape[1]\n",
    "\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "cv = ca(batch)\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "internally, pytorch flattens the tensors (B, T, D) -> (B*T, D), does the operations and then reconstructs the original dims in case of batch operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why one head is not enough\n",
    "\n",
    "- Limited subspace: A single attention head attends in a fixed representation space. It can't simultaneously focus on diverse semantic/structural patterns (like subject-object relationships and polysemy resolution).\n",
    "\n",
    "- Capacity bottleneck: One head has fewer parameters and thus limited capacity to model all the nuances. Multi-head splits the job into specialized roles — one does syntax, another semantics, etc.\n",
    "\n",
    "- Inductive bias: Multi-head attention acts like an ensemble. Each head learns a different perspective. This enforces modularity and improves generalization.\n",
    "\n",
    "- Empirical evidence: Multi-head models like Transformers perform significantly better than single-head counterparts on benchmarks (e.g., BLEU for translation, GLUE for NLP tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        keys = self.W_key(x)\n",
    "        values = self.W_value(x)\n",
    "        queries = self.W_query(x)\n",
    "\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)       # splitting the matrix in order to\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2)   # generate (B, NUM_HEADS) independent matrices\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim).transpose(1, 2) # of size (NUM_TOKENS, HEAD_DIM) for parallel\n",
    "                                                                                             # computation (hence, multi-head attention)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "        mask = torch.triu(torch.ones(num_tokens, num_tokens, device=x.device), diagonal=1).bool()\n",
    "        attn_scores = attn_scores.masked_fill(mask.unsqueeze(0).unsqueeze(0), float('-inf'))\n",
    "        print(attn_scores.shape)\n",
    "        print(attn_scores)\n",
    "        \n",
    "\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1] ** 0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context_vector = (attn_weights @ values).transpose(1, 2)\n",
    "        context_vector = context_vector.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vector = self.out_proj(context_vector)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 3, 6, 6])\n",
      "tensor([[[[-0.0501,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0193, -0.1214,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0191, -0.1195, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0098, -0.0593, -0.0568, -0.0385,    -inf,    -inf],\n",
      "          [ 0.0080, -0.0360, -0.0336, -0.0225,  0.0316,    -inf],\n",
      "          [-0.0195, -0.0856, -0.0823, -0.0559,  0.0072, -0.0949]],\n",
      "\n",
      "         [[ 0.1333,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1528,  0.1335,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1490,  0.1287,  0.1217,    -inf,    -inf,    -inf],\n",
      "          [ 0.1227,  0.1178,  0.1123,  0.0234,    -inf,    -inf],\n",
      "          [ 0.0925,  0.0716,  0.0672, -0.0339, -0.1419,    -inf],\n",
      "          [ 0.1455,  0.1444,  0.1379,  0.0418, -0.1499,  0.1617]],\n",
      "\n",
      "         [[-0.0107,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1747,  0.0356,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1752,  0.0386,  0.0356,    -inf,    -inf,    -inf],\n",
      "          [ 0.1520,  0.1042,  0.1015,  0.0930,    -inf,    -inf],\n",
      "          [ 0.1447,  0.0315,  0.0291,  0.0597,  0.1439,    -inf],\n",
      "          [ 0.1608,  0.1171,  0.1142,  0.1013,  0.1846,  0.0659]]],\n",
      "\n",
      "\n",
      "        [[[-0.0501,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0193, -0.1214,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0191, -0.1195, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0098, -0.0593, -0.0568, -0.0385,    -inf,    -inf],\n",
      "          [ 0.0080, -0.0360, -0.0336, -0.0225,  0.0316,    -inf],\n",
      "          [-0.0195, -0.0856, -0.0823, -0.0559,  0.0072, -0.0949]],\n",
      "\n",
      "         [[ 0.1333,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1528,  0.1335,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1490,  0.1287,  0.1217,    -inf,    -inf,    -inf],\n",
      "          [ 0.1227,  0.1178,  0.1123,  0.0234,    -inf,    -inf],\n",
      "          [ 0.0925,  0.0716,  0.0672, -0.0339, -0.1419,    -inf],\n",
      "          [ 0.1455,  0.1444,  0.1379,  0.0418, -0.1499,  0.1617]],\n",
      "\n",
      "         [[-0.0107,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1747,  0.0356,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1752,  0.0386,  0.0356,    -inf,    -inf,    -inf],\n",
      "          [ 0.1520,  0.1042,  0.1015,  0.0930,    -inf,    -inf],\n",
      "          [ 0.1447,  0.0315,  0.0291,  0.0597,  0.1439,    -inf],\n",
      "          [ 0.1608,  0.1171,  0.1142,  0.1013,  0.1846,  0.0659]]],\n",
      "\n",
      "\n",
      "        [[[-0.0501,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0193, -0.1214,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0191, -0.1195, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0098, -0.0593, -0.0568, -0.0385,    -inf,    -inf],\n",
      "          [ 0.0080, -0.0360, -0.0336, -0.0225,  0.0316,    -inf],\n",
      "          [-0.0195, -0.0856, -0.0823, -0.0559,  0.0072, -0.0949]],\n",
      "\n",
      "         [[ 0.1333,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1528,  0.1335,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1490,  0.1287,  0.1217,    -inf,    -inf,    -inf],\n",
      "          [ 0.1227,  0.1178,  0.1123,  0.0234,    -inf,    -inf],\n",
      "          [ 0.0925,  0.0716,  0.0672, -0.0339, -0.1419,    -inf],\n",
      "          [ 0.1455,  0.1444,  0.1379,  0.0418, -0.1499,  0.1617]],\n",
      "\n",
      "         [[-0.0107,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1747,  0.0356,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1752,  0.0386,  0.0356,    -inf,    -inf,    -inf],\n",
      "          [ 0.1520,  0.1042,  0.1015,  0.0930,    -inf,    -inf],\n",
      "          [ 0.1447,  0.0315,  0.0291,  0.0597,  0.1439,    -inf],\n",
      "          [ 0.1608,  0.1171,  0.1142,  0.1013,  0.1846,  0.0659]]],\n",
      "\n",
      "\n",
      "        [[[-0.0501,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0193, -0.1214,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0191, -0.1195, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0098, -0.0593, -0.0568, -0.0385,    -inf,    -inf],\n",
      "          [ 0.0080, -0.0360, -0.0336, -0.0225,  0.0316,    -inf],\n",
      "          [-0.0195, -0.0856, -0.0823, -0.0559,  0.0072, -0.0949]],\n",
      "\n",
      "         [[ 0.1333,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1528,  0.1335,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1490,  0.1287,  0.1217,    -inf,    -inf,    -inf],\n",
      "          [ 0.1227,  0.1178,  0.1123,  0.0234,    -inf,    -inf],\n",
      "          [ 0.0925,  0.0716,  0.0672, -0.0339, -0.1419,    -inf],\n",
      "          [ 0.1455,  0.1444,  0.1379,  0.0418, -0.1499,  0.1617]],\n",
      "\n",
      "         [[-0.0107,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1747,  0.0356,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1752,  0.0386,  0.0356,    -inf,    -inf,    -inf],\n",
      "          [ 0.1520,  0.1042,  0.1015,  0.0930,    -inf,    -inf],\n",
      "          [ 0.1447,  0.0315,  0.0291,  0.0597,  0.1439,    -inf],\n",
      "          [ 0.1608,  0.1171,  0.1142,  0.1013,  0.1846,  0.0659]]],\n",
      "\n",
      "\n",
      "        [[[-0.0501,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0193, -0.1214,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0191, -0.1195, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0098, -0.0593, -0.0568, -0.0385,    -inf,    -inf],\n",
      "          [ 0.0080, -0.0360, -0.0336, -0.0225,  0.0316,    -inf],\n",
      "          [-0.0195, -0.0856, -0.0823, -0.0559,  0.0072, -0.0949]],\n",
      "\n",
      "         [[ 0.1333,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1528,  0.1335,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1490,  0.1287,  0.1217,    -inf,    -inf,    -inf],\n",
      "          [ 0.1227,  0.1178,  0.1123,  0.0234,    -inf,    -inf],\n",
      "          [ 0.0925,  0.0716,  0.0672, -0.0339, -0.1419,    -inf],\n",
      "          [ 0.1455,  0.1444,  0.1379,  0.0418, -0.1499,  0.1617]],\n",
      "\n",
      "         [[-0.0107,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1747,  0.0356,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1752,  0.0386,  0.0356,    -inf,    -inf,    -inf],\n",
      "          [ 0.1520,  0.1042,  0.1015,  0.0930,    -inf,    -inf],\n",
      "          [ 0.1447,  0.0315,  0.0291,  0.0597,  0.1439,    -inf],\n",
      "          [ 0.1608,  0.1171,  0.1142,  0.1013,  0.1846,  0.0659]]],\n",
      "\n",
      "\n",
      "        [[[-0.0501,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0193, -0.1214,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0191, -0.1195, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0098, -0.0593, -0.0568, -0.0385,    -inf,    -inf],\n",
      "          [ 0.0080, -0.0360, -0.0336, -0.0225,  0.0316,    -inf],\n",
      "          [-0.0195, -0.0856, -0.0823, -0.0559,  0.0072, -0.0949]],\n",
      "\n",
      "         [[ 0.1333,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1528,  0.1335,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1490,  0.1287,  0.1217,    -inf,    -inf,    -inf],\n",
      "          [ 0.1227,  0.1178,  0.1123,  0.0234,    -inf,    -inf],\n",
      "          [ 0.0925,  0.0716,  0.0672, -0.0339, -0.1419,    -inf],\n",
      "          [ 0.1455,  0.1444,  0.1379,  0.0418, -0.1499,  0.1617]],\n",
      "\n",
      "         [[-0.0107,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1747,  0.0356,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1752,  0.0386,  0.0356,    -inf,    -inf,    -inf],\n",
      "          [ 0.1520,  0.1042,  0.1015,  0.0930,    -inf,    -inf],\n",
      "          [ 0.1447,  0.0315,  0.0291,  0.0597,  0.1439,    -inf],\n",
      "          [ 0.1608,  0.1171,  0.1142,  0.1013,  0.1846,  0.0659]]],\n",
      "\n",
      "\n",
      "        [[[-0.0501,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0193, -0.1214,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0191, -0.1195, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0098, -0.0593, -0.0568, -0.0385,    -inf,    -inf],\n",
      "          [ 0.0080, -0.0360, -0.0336, -0.0225,  0.0316,    -inf],\n",
      "          [-0.0195, -0.0856, -0.0823, -0.0559,  0.0072, -0.0949]],\n",
      "\n",
      "         [[ 0.1333,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1528,  0.1335,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1490,  0.1287,  0.1217,    -inf,    -inf,    -inf],\n",
      "          [ 0.1227,  0.1178,  0.1123,  0.0234,    -inf,    -inf],\n",
      "          [ 0.0925,  0.0716,  0.0672, -0.0339, -0.1419,    -inf],\n",
      "          [ 0.1455,  0.1444,  0.1379,  0.0418, -0.1499,  0.1617]],\n",
      "\n",
      "         [[-0.0107,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1747,  0.0356,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1752,  0.0386,  0.0356,    -inf,    -inf,    -inf],\n",
      "          [ 0.1520,  0.1042,  0.1015,  0.0930,    -inf,    -inf],\n",
      "          [ 0.1447,  0.0315,  0.0291,  0.0597,  0.1439,    -inf],\n",
      "          [ 0.1608,  0.1171,  0.1142,  0.1013,  0.1846,  0.0659]]],\n",
      "\n",
      "\n",
      "        [[[-0.0501,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0193, -0.1214,    -inf,    -inf,    -inf,    -inf],\n",
      "          [-0.0191, -0.1195, -0.1144,    -inf,    -inf,    -inf],\n",
      "          [-0.0098, -0.0593, -0.0568, -0.0385,    -inf,    -inf],\n",
      "          [ 0.0080, -0.0360, -0.0336, -0.0225,  0.0316,    -inf],\n",
      "          [-0.0195, -0.0856, -0.0823, -0.0559,  0.0072, -0.0949]],\n",
      "\n",
      "         [[ 0.1333,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1528,  0.1335,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1490,  0.1287,  0.1217,    -inf,    -inf,    -inf],\n",
      "          [ 0.1227,  0.1178,  0.1123,  0.0234,    -inf,    -inf],\n",
      "          [ 0.0925,  0.0716,  0.0672, -0.0339, -0.1419,    -inf],\n",
      "          [ 0.1455,  0.1444,  0.1379,  0.0418, -0.1499,  0.1617]],\n",
      "\n",
      "         [[-0.0107,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1747,  0.0356,    -inf,    -inf,    -inf,    -inf],\n",
      "          [ 0.1752,  0.0386,  0.0356,    -inf,    -inf,    -inf],\n",
      "          [ 0.1520,  0.1042,  0.1015,  0.0930,    -inf,    -inf],\n",
      "          [ 0.1447,  0.0315,  0.0291,  0.0597,  0.1439,    -inf],\n",
      "          [ 0.1608,  0.1171,  0.1142,  0.1013,  0.1846,  0.0659]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "tensor([[[ 0.0986, -0.3854, -0.0164, -0.0939, -0.3270, -0.3457],\n",
      "         [ 0.1488, -0.3594,  0.0660, -0.0833, -0.3392, -0.2930],\n",
      "         [ 0.1642, -0.3490,  0.0915, -0.0770, -0.3448, -0.2751],\n",
      "         [ 0.1567, -0.3125,  0.0546, -0.0531, -0.3465, -0.2492],\n",
      "         [ 0.1477, -0.2909,  0.0302, -0.0352, -0.3467, -0.2297],\n",
      "         [ 0.1494, -0.2826,  0.0244, -0.0331, -0.3467, -0.2251]],\n",
      "\n",
      "        [[ 0.0986, -0.3854, -0.0164, -0.0939, -0.3270, -0.3457],\n",
      "         [ 0.1488, -0.3594,  0.0660, -0.0833, -0.3392, -0.2930],\n",
      "         [ 0.1642, -0.3490,  0.0915, -0.0770, -0.3448, -0.2751],\n",
      "         [ 0.1567, -0.3125,  0.0546, -0.0531, -0.3465, -0.2492],\n",
      "         [ 0.1477, -0.2909,  0.0302, -0.0352, -0.3467, -0.2297],\n",
      "         [ 0.1494, -0.2826,  0.0244, -0.0331, -0.3467, -0.2251]],\n",
      "\n",
      "        [[ 0.0986, -0.3854, -0.0164, -0.0939, -0.3270, -0.3457],\n",
      "         [ 0.1488, -0.3594,  0.0660, -0.0833, -0.3392, -0.2930],\n",
      "         [ 0.1642, -0.3490,  0.0915, -0.0770, -0.3448, -0.2751],\n",
      "         [ 0.1567, -0.3125,  0.0546, -0.0531, -0.3465, -0.2492],\n",
      "         [ 0.1477, -0.2909,  0.0302, -0.0352, -0.3467, -0.2297],\n",
      "         [ 0.1494, -0.2826,  0.0244, -0.0331, -0.3467, -0.2251]],\n",
      "\n",
      "        [[ 0.0986, -0.3854, -0.0164, -0.0939, -0.3270, -0.3457],\n",
      "         [ 0.1488, -0.3594,  0.0660, -0.0833, -0.3392, -0.2930],\n",
      "         [ 0.1642, -0.3490,  0.0915, -0.0770, -0.3448, -0.2751],\n",
      "         [ 0.1567, -0.3125,  0.0546, -0.0531, -0.3465, -0.2492],\n",
      "         [ 0.1477, -0.2909,  0.0302, -0.0352, -0.3467, -0.2297],\n",
      "         [ 0.1494, -0.2826,  0.0244, -0.0331, -0.3467, -0.2251]],\n",
      "\n",
      "        [[ 0.0986, -0.3854, -0.0164, -0.0939, -0.3270, -0.3457],\n",
      "         [ 0.1488, -0.3594,  0.0660, -0.0833, -0.3392, -0.2930],\n",
      "         [ 0.1642, -0.3490,  0.0915, -0.0770, -0.3448, -0.2751],\n",
      "         [ 0.1567, -0.3125,  0.0546, -0.0531, -0.3465, -0.2492],\n",
      "         [ 0.1477, -0.2909,  0.0302, -0.0352, -0.3467, -0.2297],\n",
      "         [ 0.1494, -0.2826,  0.0244, -0.0331, -0.3467, -0.2251]],\n",
      "\n",
      "        [[ 0.0986, -0.3854, -0.0164, -0.0939, -0.3270, -0.3457],\n",
      "         [ 0.1488, -0.3594,  0.0660, -0.0833, -0.3392, -0.2930],\n",
      "         [ 0.1642, -0.3490,  0.0915, -0.0770, -0.3448, -0.2751],\n",
      "         [ 0.1567, -0.3125,  0.0546, -0.0531, -0.3465, -0.2492],\n",
      "         [ 0.1477, -0.2909,  0.0302, -0.0352, -0.3467, -0.2297],\n",
      "         [ 0.1494, -0.2826,  0.0244, -0.0331, -0.3467, -0.2251]],\n",
      "\n",
      "        [[ 0.0986, -0.3854, -0.0164, -0.0939, -0.3270, -0.3457],\n",
      "         [ 0.1488, -0.3594,  0.0660, -0.0833, -0.3392, -0.2930],\n",
      "         [ 0.1642, -0.3490,  0.0915, -0.0770, -0.3448, -0.2751],\n",
      "         [ 0.1567, -0.3125,  0.0546, -0.0531, -0.3465, -0.2492],\n",
      "         [ 0.1477, -0.2909,  0.0302, -0.0352, -0.3467, -0.2297],\n",
      "         [ 0.1494, -0.2826,  0.0244, -0.0331, -0.3467, -0.2251]],\n",
      "\n",
      "        [[ 0.0986, -0.3854, -0.0164, -0.0939, -0.3270, -0.3457],\n",
      "         [ 0.1488, -0.3594,  0.0660, -0.0833, -0.3392, -0.2930],\n",
      "         [ 0.1642, -0.3490,  0.0915, -0.0770, -0.3448, -0.2751],\n",
      "         [ 0.1567, -0.3125,  0.0546, -0.0531, -0.3465, -0.2492],\n",
      "         [ 0.1477, -0.2909,  0.0302, -0.0352, -0.3467, -0.2297],\n",
      "         [ 0.1494, -0.2826,  0.0244, -0.0331, -0.3467, -0.2251]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "batch = torch.stack((inputs, inputs, inputs, inputs, inputs, inputs, inputs, inputs), dim=0)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "\n",
    "mha = MultiHeadAttention(d_in=6, d_out=6, context_length=6, dropout=0, num_heads=3)\n",
    "cv = mha(batch)\n",
    "print(cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 6, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_params = sum(a.numel() for a in mha.parameters())\n",
    "total_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first n-2 dims define how many independent matrix multiplications happen in parallel.\n",
    "\n",
    "\n",
    "Imagine you have a batch of data or multiple sequences you want to process simultaneously:\n",
    "\n",
    "Those leading dims act as batch dimensions or sequence dims.\n",
    "\n",
    "PyTorch treats each slice along those dims as an independent matrix.\n",
    "\n",
    "It runs the matrix multiplication on all these slices in parallel efficiently on GPU/CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention Example\n",
    "\n",
    "Input Sentence: \"I go to the bank to deposit money\"\n",
    "\n",
    "Each attention head processes the same input, but focuses on different relationships due to different learned projections (W_q, W_k, W_v).\n",
    "\n",
    "##### Head 1: Semantic Disambiguation\n",
    "- Focus: \"bank\" ↔ \"deposit\", \"money\"\n",
    "- Learns that \"bank\" is a financial institution, not a river bank.\n",
    "\n",
    "##### Head 2: Subject Identification\n",
    "- Focus: \"I\" ↔ \"go\", \"to\"\n",
    "- Captures who is acting — the grammatical subject.\n",
    "\n",
    "##### Head 3: Grammatical Structure\n",
    "- Focus: \"to\" ↔ \"go\", \"deposit\"\n",
    "- Models motion or action phrases — verb-preposition-object relationships.\n",
    "\n",
    "\n",
    "Multi-head attention enables different heads to learn specialized roles by attending in different representational subspaces, improving model expressiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
